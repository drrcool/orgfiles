#+title: Leave Notes

* Dates:
2/27 - 5/1 (Tenative)
During this time, I will try to remain offline to minimize external stress.  In an emergency, I can be reached at =669-977-2583=

* QoEDash
We have decided to leave QoEDash v1.0 in prod during my leave.  Version 2.0 will be available in test.
** Data Sources
*** Generic Druid ingestion job (for all but PQS):

https://stash.corp.netflix.com/projects/CSNA/repos/qoedashetl/browse/ingest_druid_data.py

This job includes the druid ingestion script for the ecosystem, playback, and appqoe tables.
*** QoEDash ETL (excluding PQS) workflow:
https://stash.corp.netflix.com/projects/CSNA/repos/qoedashetl/browse/ingest_druid_data.py
*** Ecosystem
This data set is fed primary from =dse.device_version_activity_sum= and then denormalized.
**** Agg table: rpt.qoedash_activity_sum
**** Code:
https://stash.corp.netflix.com/projects/CSNA/repos/qoedashetl/browse/qoedash_activity_sum.sql

*** Playback QoE
This data set essentially copies =dse.playback_f= and denormalizes it.  It was adjusted on 2/24 to include columns that are being deprecated from =playback_f=.  Note  that as the aggregation for metrics which use tDigests happens on ingestion, this table is a direct denormalized copy of playback_f. To keep the ingestion job from choking on the data volume, each day we kick off 24 jobs (one for each hour).
*** App QoE
*** PQS
**** Workflow:
***** https://data.netflix.net/workflows/prod:PRODUCT_QUALITY_SCORE
**** 'Fact' Table:
***** dse.product_quality_score_f
This table is generated by a pyspark job that reads multiple fact tables, typically at the grain of:
- device_type_id
- device_model
- client_version
- vesn
- account_id
- asn_num
We aggregate the fact events at the day granularity (otherwise, this fact table would essentially be a copy of playback_f, client_performance_f, and scl which is quite large).  The aggregates for each of the domains (playback, crashes, reauth etc) are then joined to create the output fact table.
****** ETL script: https://stash.corp.netflix.com/projects/CSNA/repos/device_reliability_etl/browse/pqs/product_quality_score_f.py


**** Aggregate Table
The daily aggregate table, at the same course granularity of the fact table above.

***** dse.product_quality_score_sum
***** https://stash.corp.netflix.com/projects/CSNA/repos/device_reliability_etl/browse/pqs/product_quality_score_report_sum.sql

**** Report Table
Druid doesn't do joins, so any table thats loaded to druid must be denormalized to include all the dimensions of interest.  This job is hefty and must include all business logic. *Do not assume you can add business logic to the dashboard code, this is bad practice and leads to dashboard code that is coupled to external logic.*  If someone pushes you to do this, you should push back /very hard/ as it's the easy way out that leads to spaghetti code and is the natural solution when caring about the long-term maintainability of the code base isn't a concern for the requester.  This happens a lot with qoedash.  Move this logic to the report table construction.  If at all possible, any table referenced from this code should be owned by a data engineering team and not fed from a personal schema.  If there is extremly complex logic that's being added to this code, that's a good sign that the logic should be owned by a DSE data engineer and not the dashboard owner.
***** rpt.product_quality_score_report_sum
***** https://stash.corp.netflix.com/projects/CSNA/repos/device_reliability_etl/browse/pqs/product_quality_score_report_sum.sql
***** Script to load to druid:
https://stash.corp.netflix.com/projects/CSNA/repos/device_reliability_etl/browse/pqs/load_pqs_to_druid.py
** Dashboard
*** Repo: https://stash.corp.netflix.com/projects/CSNA/repos/qoedash/browse
*** Spinnaker Application: https://www.spinnaker.mgmt.netflix.net/#/applications/qoedash/clusters
*** Triggers:
- A new test build is triggered with each code push.  Spinnaker automatically triggers off of the Jenkins job and starts a new deploy for test.
- If you need to change something in version 1.0, know that the change will **not** persist to v2.0, so you will need to make the change in both the release1.0 branch and main.  DO NOT, UNDER ANY CIRCUMSTANCES, MERGE PULL REQUESTS TO RELEASE1.0 TO MAIN.
- Changes for version 2.0 can be merged to main.  If you PR main into release 2.0, it will trigger a jenkins job and spinnaker pipeline to deploy to prod.  As long as the intent is to keep v1.0 in prod, you should only merge to main and not promote main to release 2.0.
*** Adding Dimensions:
- If added new dimensions to the ETL, care must be taken not to explode the data set. The ingestion jobs are already hefty, the data storage costs aren't small, and dashboard performance is hit significantly as we add high cardinality columns.
- Dimensions are defined in the JSON file: https://stash.corp.netflix.com/projects/CSNA/repos/qoedash/browse/src/config/dimensions/dimensionConfig.json

* Sessionwiz:
The current version of sessionWiz for gaming is not in a usable state.  Swirl around the best source of data and attempts to move the backend to a new system have resulted in a number of false starts.  In the short term, I suggest leaning into Aperture (contact: Michael Paulson).  The current version of the dashboard is failing due to cross origin errors, but I have not had time to fix them before going on leave.  Given the uncertain status on the long-term viability of the telemetry service, our original plan to move TCAT as the data source for the dashboard may not be ideal. Once that is finalized more permanent plans can be set.

* Jigsaw:
Jigsaw has largly been in maintence mode with little updates needed. That said, it was moved from Meson to Maestro just before I left on leave to aid in the migration efforts.

* Adhoc:

** App Updateability

** Cloud Gaming Reach
